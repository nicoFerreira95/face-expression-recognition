{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c68b2e-4ba7-4180-97a9-90c943bd9567",
   "metadata": {},
   "source": [
    "# Face recognition modeling\n",
    "\n",
    "In the following notebook, I will fit a variety of models to do image classification.\n",
    "\n",
    "**Selected models**:\n",
    "* Convolutional Neural Network (CNN),\n",
    "* Hugging face image classification transformer,\n",
    "* ViTs\n",
    "\n",
    "All of them will be fit in order to compare their performance and offer benchmarks.\n",
    "\n",
    "**Hyperparameter tuning**:\n",
    "* steps per epoch,\n",
    "* epoch,\n",
    "* learning rate,\n",
    "* and other parameters\n",
    "\n",
    "Will be fit using RandomSearch (NOT GridSearch :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a59e65b-0b58-4020-a61b-154e66c7bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import models, layers\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f87e41d-79af-497f-ba86-2b48142ebc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arrays_from_gcs(bucket_name, imgs_filename, lbls_filename):\n",
    "    \"\"\"\n",
    "    Reads the imgs and lbls arrays from Google Cloud Storage (GCS).\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the GCS bucket.\n",
    "        imgs_filename (str): The filename for the imgs array.\n",
    "        lbls_filename (str): The filename for the lbls array.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the imgs and lbls arrays.\n",
    "    \"\"\"\n",
    "    # Set up the GCS client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Read the .npy files from the GCS bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(imgs_filename)\n",
    "    blob.download_to_filename(imgs_filename)\n",
    "    blob = bucket.blob(lbls_filename)\n",
    "    blob.download_to_filename(lbls_filename)\n",
    "\n",
    "    # Load the .npy files into NumPy arrays\n",
    "    imgs = np.load(imgs_filename)\n",
    "    lbls = np.load(lbls_filename)\n",
    "\n",
    "    return imgs, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb05b3d8-0008-420e-8ca8-3cee703abc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"msca-ml-final-project\"\n",
    "train_imgs_filename = \"train_imgs.npy\"\n",
    "train_lbls_filename = \"train_lbls.npy\"\n",
    "test_imgs_filename = \"test_imgs.npy\"\n",
    "test_lbls_filename = \"test_lbls.npy\"\n",
    "val_imgs_filename = \"val_imgs.npy\"\n",
    "val_lbls_filename = \"val_lbls.npy\"\n",
    "\n",
    "train_imgs, train_lbls = read_arrays_from_gcs(bucket_name, train_imgs_filename, train_lbls_filename)\n",
    "test_imgs, test_lbls = read_arrays_from_gcs(bucket_name, test_imgs_filename, test_lbls_filename)\n",
    "val_imgs, val_lbls = read_arrays_from_gcs(bucket_name, val_imgs_filename, val_lbls_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa215d44-fb84-4c9e-af58-84c0ab471705",
   "metadata": {},
   "source": [
    "Print shape of all files to make sure they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d81402d-b1ff-49da-bcb1-a27a2d596d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (28709, 48, 48, 1)\n",
      "Shape of testing data: (3589, 48, 48, 1)\n",
      "Shape of validation data: (3589, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of training data: {np.shape(train_imgs)}')\n",
    "print(f'Shape of testing data: {np.shape(test_imgs)}')\n",
    "print(f'Shape of validation data: {np.shape(val_imgs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe13807-8dd0-4333-a288-5826ff66e733",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ba12f-1436-4c49-a095-b58e06f5b79c",
   "metadata": {},
   "source": [
    "#### 1. CNN - LeNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "390f2146-6f76-4756-a212-ba35f5adc74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 23:55:09.694072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-15 23:55:09.694128: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-15 23:55:09.694156: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tensorflow-ml-project): /proc/driver/nvidia/version does not exist\n",
      "2023-05-15 23:55:09.695770: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 44, 44, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 22, 22, 6)        0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 18, 18, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 9, 9, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1296)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               155640    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 595       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,971\n",
      "Trainable params: 168,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining LeNet architecture\n",
    "def create_lenet_model(input_shape, num_classes):\n",
    "    model_lenet = Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    model_lenet.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model_lenet.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model_lenet.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "    model_lenet.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # Flatten the 3D output to 1D\n",
    "    model_lenet.add(Flatten())\n",
    "    \n",
    "    # Fully connected layers\n",
    "    model_lenet.add(Dense(120, activation='relu'))\n",
    "    model_lenet.add(Dense(84, activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model_lenet.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model_lenet\n",
    "\n",
    "# Create an instance of the LeNet model\n",
    "input_shape = (48, 48, 1)  # Input shape of your images\n",
    "num_classes = 7  # Number of classes for classification\n",
    "model_lenet = create_lenet_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model_lenet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f297aec-d40d-42f6-8bc6-25b06be426b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model, and validating\n",
    "model_lenet.fit(train_imgs, train_lbls, \n",
    "          epochs=5, batch_size=32, \n",
    "          validation_data=(val_imgs, val_lbls), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
